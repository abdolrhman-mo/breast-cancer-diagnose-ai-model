{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6306db8",
   "metadata": {},
   "source": [
    "## **Phase 8: Model Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "23eee38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f3a86a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "X_train_scaled = joblib.load(\"../data/processed/X_train_scaled.pkl\")\n",
    "y_train = joblib.load(\"../data/processed/y_train.pkl\")\n",
    "X_test_scaled = joblib.load(\"../data/processed/X_test_scaled.pkl\")\n",
    "y_test = joblib.load(\"../data/processed/y_test.pkl\")\n",
    "\n",
    "lr_model = joblib.load(\"../models/logistic_regression.pkl\")\n",
    "svm_model = joblib.load(\"../models/svm.pkl\")\n",
    "rf_model = joblib.load(\"../models/random_forest.pkl\")\n",
    "xgb_model = joblib.load(\"../models/xgboost.pkl\")\n",
    "voting_model = joblib.load(\"../models/voting_classifier.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fb646571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred_lr = lr_model.predict(X_test_scaled)\n",
    "y_pred_svm = svm_model.predict(X_test_scaled)\n",
    "y_pred_xgb = xgb_model.predict(X_test_scaled)\n",
    "y_pred_rf = rf_model.predict(X_test_scaled)\n",
    "y_pred_voting = voting_model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6b1a0c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ðŸ“Š MODEL EVALUATION RESULTS\n",
      "============================================================\n",
      "\n",
      "LOGISTIC REGRESSION\n",
      "----------------------------------------\n",
      "Recall:    0.9286\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.97        72\n",
      "           1       0.97      0.93      0.95        42\n",
      "\n",
      "    accuracy                           0.96       114\n",
      "   macro avg       0.97      0.96      0.96       114\n",
      "weighted avg       0.97      0.96      0.96       114\n",
      "\n",
      "\n",
      "SVM\n",
      "----------------------------------------\n",
      "Recall:    0.9048\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97        72\n",
      "           1       1.00      0.90      0.95        42\n",
      "\n",
      "    accuracy                           0.96       114\n",
      "   macro avg       0.97      0.95      0.96       114\n",
      "weighted avg       0.97      0.96      0.96       114\n",
      "\n",
      "\n",
      "XGBOOST\n",
      "----------------------------------------\n",
      "Recall:    0.9048\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97        72\n",
      "           1       1.00      0.90      0.95        42\n",
      "\n",
      "    accuracy                           0.96       114\n",
      "   macro avg       0.97      0.95      0.96       114\n",
      "weighted avg       0.97      0.96      0.96       114\n",
      "\n",
      "\n",
      "RANDOM FOREST\n",
      "----------------------------------------\n",
      "Recall:    0.9286\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98        72\n",
      "           1       1.00      0.93      0.96        42\n",
      "\n",
      "    accuracy                           0.97       114\n",
      "   macro avg       0.98      0.96      0.97       114\n",
      "weighted avg       0.97      0.97      0.97       114\n",
      "\n",
      "\n",
      "VOTING CLASSIFIER\n",
      "----------------------------------------\n",
      "Recall:    0.9286\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98        72\n",
      "           1       1.00      0.93      0.96        42\n",
      "\n",
      "    accuracy                           0.97       114\n",
      "   macro avg       0.98      0.96      0.97       114\n",
      "weighted avg       0.97      0.97      0.97       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluation function for cleaner code\n",
    "def evaluate_model(model, y_true, y_pred, model_name, X_test_data):\n",
    "    \"\"\"Evaluate a single model and return metrics\"\"\"\n",
    "    \n",
    "    # Basic metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, pos_label=1)\n",
    "    recall = recall_score(y_true, y_pred, pos_label=1)\n",
    "    f1 = f1_score(y_true, y_pred, pos_label=1)\n",
    "    \n",
    "    return {\n",
    "        'Model': model_name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "    }\n",
    "\n",
    "# Evaluate all models\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸ“Š MODEL EVALUATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Evaluate each model\n",
    "lr_results = evaluate_model(lr_model, y_test, y_pred_lr, \"Logistic Regression\", X_test_scaled)\n",
    "svm_results = evaluate_model(svm_model, y_test, y_pred_svm, \"SVM\", X_test_scaled)\n",
    "xgb_results = evaluate_model(xgb_model, y_test, y_pred_xgb, \"XGBoost\", X_test_scaled)\n",
    "rf_results = evaluate_model(rf_model, y_test, y_pred_rf, \"Random Forest\", X_test_scaled)\n",
    "voting_results = evaluate_model(voting_model, y_test, y_pred_voting, \"Voting Classifier\", X_test_scaled)\n",
    "\n",
    "# Display detailed results for each model\n",
    "models_to_evaluate = [lr_results, svm_results, xgb_results, rf_results, voting_results]\n",
    "\n",
    "for model_result in models_to_evaluate:\n",
    "    print(f\"\\n{model_result['Model'].upper()}\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Recall:    {model_result['Recall']:.4f}\")\n",
    "    # print(f\"Accuracy:  {model_result['Accuracy']:.4f}\")\n",
    "    # print(f\"Precision: {model_result['Precision']:.4f}\")\n",
    "    # print(f\"F1 Score:  {model_result['F1 Score']:.4f}\")\n",
    "    # print(f\"ROC AUC:   {model_result['ROC AUC']:.4f}\")\n",
    "    \n",
    "    # Get predictions for classification report\n",
    "    if model_result['Model'] == \"Logistic Regression\":\n",
    "        y_pred_for_report = y_pred_lr\n",
    "    elif model_result['Model'] == \"SVM\":\n",
    "        y_pred_for_report = y_pred_svm\n",
    "    elif model_result['Model'] == \"XGBoost\":\n",
    "        y_pred_for_report = y_pred_xgb\n",
    "    else:  # Random Forest\n",
    "        y_pred_for_report = y_pred_rf\n",
    "    \n",
    "    print(f\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred_for_report))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
