{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f3d0f9a",
   "metadata": {},
   "source": [
    "**Phases 8-9: evaluation, and optimization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6306db8",
   "metadata": {},
   "source": [
    "## **Phase 8: Model Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "23eee38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1f3a86a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "X_train_scaled = joblib.load(\"../data/processed/X_train_scaled.pkl\")\n",
    "y_train = joblib.load(\"../data/processed/y_train.pkl\")\n",
    "X_test_scaled = joblib.load(\"../data/processed/X_test_scaled.pkl\")\n",
    "y_test = joblib.load(\"../data/processed/y_test.pkl\")\n",
    "\n",
    "lr_model = joblib.load(\"../models/logistic_regression.pkl\")\n",
    "svm_model = joblib.load(\"../models/svm.pkl\")\n",
    "rf_model = joblib.load(\"../models/random_forest.pkl\")\n",
    "xgb_model = joblib.load(\"../models/xgboost.pkl\")\n",
    "voting_model = joblib.load(\"../models/voting_classifier.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fb646571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred_lr = lr_model.predict(X_test_scaled)\n",
    "y_pred_svm = svm_model.predict(X_test_scaled)\n",
    "y_pred_xgb = xgb_model.predict(X_test_scaled)\n",
    "y_pred_rf = rf_model.predict(X_test_scaled)\n",
    "y_pred_voting = voting_model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6b1a0c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "📊 MODEL EVALUATION RESULTS\n",
      "============================================================\n",
      "\n",
      "LOGISTIC REGRESSION\n",
      "----------------------------------------\n",
      "Recall:    0.9286\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.97        72\n",
      "           1       0.97      0.93      0.95        42\n",
      "\n",
      "    accuracy                           0.96       114\n",
      "   macro avg       0.97      0.96      0.96       114\n",
      "weighted avg       0.97      0.96      0.96       114\n",
      "\n",
      "\n",
      "SVM\n",
      "----------------------------------------\n",
      "Recall:    0.9048\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97        72\n",
      "           1       1.00      0.90      0.95        42\n",
      "\n",
      "    accuracy                           0.96       114\n",
      "   macro avg       0.97      0.95      0.96       114\n",
      "weighted avg       0.97      0.96      0.96       114\n",
      "\n",
      "\n",
      "XGBOOST\n",
      "----------------------------------------\n",
      "Recall:    0.9048\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97        72\n",
      "           1       1.00      0.90      0.95        42\n",
      "\n",
      "    accuracy                           0.96       114\n",
      "   macro avg       0.97      0.95      0.96       114\n",
      "weighted avg       0.97      0.96      0.96       114\n",
      "\n",
      "\n",
      "RANDOM FOREST\n",
      "----------------------------------------\n",
      "Recall:    0.9286\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98        72\n",
      "           1       1.00      0.93      0.96        42\n",
      "\n",
      "    accuracy                           0.97       114\n",
      "   macro avg       0.98      0.96      0.97       114\n",
      "weighted avg       0.97      0.97      0.97       114\n",
      "\n",
      "\n",
      "VOTING CLASSIFIER\n",
      "----------------------------------------\n",
      "Recall:    0.9286\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98        72\n",
      "           1       1.00      0.93      0.96        42\n",
      "\n",
      "    accuracy                           0.97       114\n",
      "   macro avg       0.98      0.96      0.97       114\n",
      "weighted avg       0.97      0.97      0.97       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluation function for cleaner code\n",
    "def evaluate_model(model, y_true, y_pred, model_name, X_test_data):\n",
    "    \"\"\"Evaluate a single model and return metrics\"\"\"\n",
    "    \n",
    "    # Basic metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, pos_label=1)\n",
    "    recall = recall_score(y_true, y_pred, pos_label=1)\n",
    "    f1 = f1_score(y_true, y_pred, pos_label=1)\n",
    "    \n",
    "    return {\n",
    "        'Model': model_name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "    }\n",
    "\n",
    "# Evaluate all models\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"📊 MODEL EVALUATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Evaluate each model\n",
    "lr_results = evaluate_model(lr_model, y_test, y_pred_lr, \"Logistic Regression\", X_test_scaled)\n",
    "svm_results = evaluate_model(svm_model, y_test, y_pred_svm, \"SVM\", X_test_scaled)\n",
    "xgb_results = evaluate_model(xgb_model, y_test, y_pred_xgb, \"XGBoost\", X_test_scaled)\n",
    "rf_results = evaluate_model(rf_model, y_test, y_pred_rf, \"Random Forest\", X_test_scaled)\n",
    "voting_results = evaluate_model(voting_model, y_test, y_pred_voting, \"Voting Classifier\", X_test_scaled)\n",
    "\n",
    "# Display detailed results for each model\n",
    "models_to_evaluate = [lr_results, svm_results, xgb_results, rf_results, voting_results]\n",
    "\n",
    "for model_result in models_to_evaluate:\n",
    "    print(f\"\\n{model_result['Model'].upper()}\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Recall:    {model_result['Recall']:.4f}\")\n",
    "    # print(f\"Accuracy:  {model_result['Accuracy']:.4f}\")\n",
    "    # print(f\"Precision: {model_result['Precision']:.4f}\")\n",
    "    # print(f\"F1 Score:  {model_result['F1 Score']:.4f}\")\n",
    "    # print(f\"ROC AUC:   {model_result['ROC AUC']:.4f}\")\n",
    "    \n",
    "    # Get predictions for classification report\n",
    "    if model_result['Model'] == \"Logistic Regression\":\n",
    "        y_pred_for_report = y_pred_lr\n",
    "    elif model_result['Model'] == \"SVM\":\n",
    "        y_pred_for_report = y_pred_svm\n",
    "    elif model_result['Model'] == \"XGBoost\":\n",
    "        y_pred_for_report = y_pred_xgb\n",
    "    else:  # Random Forest\n",
    "        y_pred_for_report = y_pred_rf\n",
    "    \n",
    "    print(f\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred_for_report))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb6aa70",
   "metadata": {},
   "source": [
    "## **Phase 9: Model Optimization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "93863aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "🎯 SVM THRESHOLD OPTIMIZATION - FINDING BEST RECALL\n",
      "============================================================\n",
      "Threshold | Recall  | Precision | F1-Score\n",
      "---------------------------------------------\n",
      "   0.1    | 0.9762  | 0.8200    | 0.8913\n",
      "   0.2    | 0.9762  | 0.9762    | 0.9762\n",
      "   0.3    | 0.9762  | 1.0000    | 0.9880\n",
      "   0.4    | 0.9762  | 1.0000    | 0.9880\n",
      "   0.5    | 0.9286  | 1.0000    | 0.9630\n",
      "   0.6    | 0.8810  | 1.0000    | 0.9367\n",
      "   0.7    | 0.8333  | 1.0000    | 0.9091\n",
      "   0.8    | 0.7857  | 1.0000    | 0.8800\n",
      "   0.9    | 0.7619  | 1.0000    | 0.8649\n",
      "\n",
      "🏆 BEST SVM THRESHOLD: 0.1\n",
      "   Recall: 0.9762\n",
      "   Precision: 0.8200\n",
      "   F1-Score: 0.8913\n",
      "\n",
      "🔴 HIGH-RISK CUSTOMERS (Probability >= 0.1): 50\n",
      "✅ SVM threshold optimization completed!\n",
      "💡 Use threshold 0.1 for production deployment\n"
     ]
    }
   ],
   "source": [
    "# Simple SVM threshold optimization\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"🎯 SVM THRESHOLD OPTIMIZATION - FINDING BEST RECALL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get SVM probabilities (using the same data format as your existing code)\n",
    "voting_probs = voting_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Test different thresholds and find best recall\n",
    "thresholds = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "best_recall = 0\n",
    "best_threshold = 0.5\n",
    "best_metrics = {}\n",
    "\n",
    "print(\"Threshold | Recall  | Precision | F1-Score\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "for threshold in thresholds:\n",
    "    # Apply threshold\n",
    "    predictions = (voting_probs >= threshold).astype(int)\n",
    "    pred_labels = [1 if pred == 1 else 0 for pred in predictions]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    recall = recall_score(y_test, pred_labels, pos_label=1)\n",
    "    precision = precision_score(y_test, pred_labels, pos_label=1)\n",
    "    f1 = f1_score(y_test, pred_labels, pos_label=1)\n",
    "    \n",
    "    print(f\"{threshold:^9} | {recall:.4f}  | {precision:.4f}    | {f1:.4f}\")\n",
    "    \n",
    "    # Track best recall\n",
    "    if recall > best_recall:\n",
    "        best_recall = recall\n",
    "        best_threshold = threshold\n",
    "        best_metrics = {'recall': recall, 'precision': precision, 'f1': f1}\n",
    "\n",
    "print(f\"\\n🏆 BEST SVM THRESHOLD: {best_threshold}\")\n",
    "print(f\"   Recall: {best_metrics['recall']:.4f}\")\n",
    "print(f\"   Precision: {best_metrics['precision']:.4f}\")\n",
    "print(f\"   F1-Score: {best_metrics['f1']:.4f}\")\n",
    "\n",
    "# Show high-risk customers\n",
    "high_risk_count = (svm_probs >= best_threshold).sum()\n",
    "print(f\"\\n🔴 HIGH-RISK CUSTOMERS (Probability >= {best_threshold}): {high_risk_count}\")\n",
    "\n",
    "print(\"✅ SVM threshold optimization completed!\")\n",
    "print(f\"💡 Use threshold {best_threshold} for production deployment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac233f8",
   "metadata": {},
   "source": [
    "## Saving the Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f568bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the optimized SVM model with best threshold\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"💾 SAVING OPTIMIZED VOTING MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create models directory if it doesn't exist\n",
    "models_path = \"../models\"\n",
    "os.makedirs(models_path, exist_ok=True)\n",
    "\n",
    "# Save the Voting model\n",
    "voting_model_path = f\"{models_path}/voting_model.pkl\"\n",
    "joblib.dump(voting_model, voting_model_path)\n",
    "print(f\"✅ Voting model saved to: {voting_model_path}\")\n",
    "\n",
    "# Save the best threshold value\n",
    "threshold_path = f\"{models_path}/voting_threshold.txt\"\n",
    "with open(threshold_path, 'w') as f:\n",
    "    f.write(str(best_threshold))\n",
    "print(f\"✅ Best threshold saved to: {threshold_path}\")\n",
    "\n",
    "# Save the scaler for preprocessing new data\n",
    "scaler_path = f\"{models_path}/scaler.pkl\"\n",
    "joblib.dump(scaler, scaler_path)\n",
    "print(f\"✅ Scaler saved to: {scaler_path}\")\n",
    "\n",
    "# Save the preprocessing pipeline\n",
    "processing_path = f\"{models_path}/preprocessing.pkl\"\n",
    "joblib.dump(processing, processing_path)\n",
    "print(f\"✅ Preprocessing pipeline saved to: {processing_path}\")\n",
    "\n",
    "print(f\"\\n🎯 Model deployment ready!\")\n",
    "print(f\"   - Use threshold: {best_threshold}\")\n",
    "print(f\"   - Model file: {svm_model_path}\")\n",
    "print(f\"   - For new predictions: load model and apply threshold {best_threshold}\")\n",
    "\n",
    "# Function to load and use the saved model\n",
    "def load_and_predict_svm(customer_data):\n",
    "    \"\"\"\n",
    "    Load the saved SVM model and make predictions on new customer data\n",
    "    \n",
    "    Parameters:\n",
    "    customer_data: DataFrame with customer features (same columns as training data)\n",
    "    \n",
    "    Returns:\n",
    "    predictions: Churn predictions (Yes/No)\n",
    "    probabilities: Churn probabilities\n",
    "    risk_scores: Risk scores based on threshold\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the saved components\n",
    "        svm_model = joblib.load(f\"{models_path}/svm_model.pkl\")\n",
    "        scaler = joblib.load(f\"{models_path}/scaler.pkl\")\n",
    "        processing = joblib.load(f\"{models_path}/preprocessing.pkl\")\n",
    "        \n",
    "        # Load the best threshold\n",
    "        with open(f\"{models_path}/svm_threshold.txt\", 'r') as f:\n",
    "            best_threshold = float(f.read().strip())\n",
    "        \n",
    "        print(f\"✅ Model loaded successfully with threshold: {best_threshold}\")\n",
    "        \n",
    "        # Preprocess the new data\n",
    "        customer_processed = processing.transform(customer_data)\n",
    "        customer_scaled = scaler.transform(customer_processed)\n",
    "        \n",
    "        # Get probabilities\n",
    "        probabilities = svm_model.predict_proba(customer_scaled)[:, 1]\n",
    "        \n",
    "        # Apply the optimized threshold\n",
    "        predictions = ['Yes' if prob >= best_threshold else 'No' for prob in probabilities]\n",
    "        \n",
    "        # Create risk scores\n",
    "        risk_scores = []\n",
    "        for prob in probabilities:\n",
    "            if prob >= best_threshold:\n",
    "                if prob >= 0.8:\n",
    "                    risk_scores.append('Very High Risk')\n",
    "                elif prob >= 0.6:\n",
    "                    risk_scores.append('High Risk')\n",
    "                else:\n",
    "                    risk_scores.append('Medium Risk')\n",
    "            else:\n",
    "                if prob <= 0.2:\n",
    "                    risk_scores.append('Very Low Risk')\n",
    "                else:\n",
    "                    risk_scores.append('Low Risk')\n",
    "        \n",
    "        return predictions, probabilities, risk_scores\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading model: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "# Example usage (uncomment to test)\n",
    "# print(\"\\n\" + \"=\"*60)\n",
    "# print(\"🧪 TESTING MODEL LOADING\")\n",
    "# print(\"=\"*60)\n",
    "# \n",
    "# # Create sample customer data (replace with your actual data)\n",
    "# sample_customer = pd.DataFrame({\n",
    "#     'gender': ['Male'],\n",
    "#     'SeniorCitizen': [0],\n",
    "#     'Partner': ['Yes'],\n",
    "#     'Dependents': ['No'],\n",
    "#     'tenure': [12],\n",
    "#     'PhoneService': ['Yes'],\n",
    "#     'InternetService': ['DSL'],\n",
    "#     'OnlineSecurity': ['No'],\n",
    "#     'OnlineBackup': ['No'],\n",
    "#     'DeviceProtection': ['No'],\n",
    "#     'TechSupport': ['No'],\n",
    "#     'StreamingTV': ['No'],\n",
    "#     'StreamingMovies': ['No'],\n",
    "#     'Contract': ['Month-to-month'],\n",
    "#     'MonthlyCharges': [65.0],\n",
    "#     'TotalCharges': [780.0]\n",
    "# })\n",
    "# \n",
    "# # Make prediction\n",
    "# predictions, probabilities, risk_scores = load_and_predict_svm(sample_customer)\n",
    "# \n",
    "# if predictions is not None:\n",
    "#     print(f\"Sample customer prediction: {predictions[0]}\")\n",
    "#     print(f\"Churn probability: {probabilities[0]:.3f}\")\n",
    "#     print(f\"Risk level: {risk_scores[0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
